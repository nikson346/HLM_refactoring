{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import EMdata\n",
    "import torch\n",
    "import itertools\n",
    "import random, math\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU or CPU\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path\n",
    "file_path='F:/script/class2vec/real_star_file/10943_subset1_major.star'\n",
    "datatype=0 #0 is relion 3.1, 1 is relion 3, 2 is cryosparc\n",
    "fast=False\n",
    "block_size=66\n",
    "file_name=os.path.basename(file_path)\n",
    "output_path=os.path.dirname(file_path)+'/'+os.path.splitext(file_name)[0]\n",
    "if os.path.isdir(output_path) is False:\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_rlnCoordinateX', '_rlnCoordinateY', '_rlnHelicalTubeID', '_rlnAngleTiltPrior', '_rlnAnglePsiPrior', '_rlnHelicalTrackLengthAngst', '_rlnAnglePsiFlipRatio', '_rlnImageName', '_rlnMicrographName', '_rlnOpticsGroup', '_rlnCtfMaxResolution', '_rlnCtfFigureOfMerit', '_rlnDefocusU', '_rlnDefocusV', '_rlnDefocusAngle', '_rlnCtfBfactor', '_rlnCtfScalefactor', '_rlnPhaseShift', '_rlnGroupNumber', '_rlnAngleRot', '_rlnAngleTilt', '_rlnAnglePsi', '_rlnOriginXAngst', '_rlnOriginYAngst', '_rlnClassNumber', '_rlnNormCorrection', '_rlnLogLikeliContribution', '_rlnMaxValueProbDistribution', '_rlnNrOfSignificantSamples']\n",
      "['3577.521329', '2876.543431', '6', '90.000000', '-99.81930', '57.000000', '0.500000', '000001@Extract/job087/data/000031665775756168679_FoilHole_21951491_Data_21939126_21939128_20220223_233928_EER_patch_aligned_doseweighted.mrcs', 'data/000031665775756168679_FoilHole_21951491_Data_21939126_21939128_20220223_233928_EER_patch_aligned_doseweighted.mrc', '1', '7.478750', '0.021992', '12399.541016', '12399.541016', '46.683113', '0.000000', '1.000000', '0.000000', '1', '0.000000', '90.000000', '-95.04146', '-2.33296', '-1.05059', '80', '0.557517', '1.664881e+05', '0.667336', '4']\n",
      "finish reading\n",
      "number of particles 1032739\n",
      "0 0.32968298594156903 mins\n",
      "10000 0.3328170816103617 mins\n",
      "20000 0.3421188712120056 mins\n",
      "30000 0.35710580348968507 mins\n",
      "40000 0.37762708266576134 mins\n",
      "50000 0.4037662943204244 mins\n",
      "60000 0.4355567693710327 mins\n",
      "70000 0.4732985893885295 mins\n",
      "80000 0.5165416439374287 mins\n",
      "90000 0.5647858222325642 mins\n",
      "100000 0.6195314645767211 mins\n",
      "110000 0.6805118441581726 mins\n",
      "120000 0.7490939259529114 mins\n",
      "130000 0.8259445389111837 mins\n",
      "140000 0.911197038491567 mins\n",
      "150000 1.0068852146466574 mins\n",
      "160000 1.1144427259763081 mins\n",
      "170000 1.2343363523483277 mins\n",
      "180000 1.3718339284261067 mins\n",
      "190000 1.5207674463589986 mins\n",
      "200000 1.6864045540491739 mins\n",
      "210000 1.8663784424463907 mins\n",
      "220000 2.058921710650126 mins\n",
      "230000 2.2620007475217183 mins\n",
      "240000 2.4393239418665567 mins\n",
      "250000 2.6179080486297606 mins\n",
      "260000 2.8030162930488585 mins\n",
      "270000 3.0064287145932513 mins\n",
      "280000 3.230762469768524 mins\n",
      "290000 3.4777977228164674 mins\n",
      "300000 3.747741711139679 mins\n",
      "310000 4.039657374223073 mins\n",
      "320000 4.348376778761546 mins\n",
      "330000 4.678117593129476 mins\n",
      "340000 5.021911541620891 mins\n",
      "350000 5.381392355759939 mins\n",
      "360000 5.754876327514649 mins\n",
      "370000 6.137062187989553 mins\n",
      "380000 6.53523508310318 mins\n",
      "390000 6.948761383692424 mins\n",
      "400000 7.3741236885388695 mins\n",
      "410000 7.815372876326243 mins\n",
      "420000 8.253371103604634 mins\n",
      "430000 8.70090529123942 mins\n",
      "440000 9.167060089111327 mins\n",
      "450000 9.649818634986877 mins\n",
      "460000 10.143079467614491 mins\n",
      "470000 10.651127056280773 mins\n",
      "480000 11.173061072826385 mins\n",
      "490000 11.70829806725184 mins\n",
      "500000 12.254637336730957 mins\n",
      "510000 12.81628049214681 mins\n",
      "520000 13.3914097905159 mins\n",
      "530000 13.983276196320851 mins\n",
      "540000 14.584227951367696 mins\n",
      "550000 15.191481141249339 mins\n",
      "560000 15.811553875605265 mins\n",
      "570000 16.447646939754485 mins\n",
      "580000 17.090124646822613 mins\n",
      "590000 17.74828929901123 mins\n",
      "600000 18.412938702106477 mins\n",
      "610000 19.086140052477518 mins\n",
      "620000 19.777028711636863 mins\n",
      "630000 20.47783626715342 mins\n",
      "640000 21.1889794866244 mins\n",
      "650000 21.90582396586736 mins\n",
      "660000 22.635681462287902 mins\n",
      "670000 23.37455096244812 mins\n",
      "680000 24.12958735624949 mins\n",
      "690000 24.89592631657918 mins\n",
      "700000 25.67776874701182 mins\n",
      "710000 26.462328461805978 mins\n",
      "720000 27.260491240024567 mins\n",
      "730000 28.068339320023856 mins\n",
      "740000 28.893641750017803 mins\n",
      "750000 29.728829487164816 mins\n",
      "760000 30.584355227152507 mins\n",
      "770000 31.443381615479787 mins\n",
      "780000 32.31491088072459 mins\n",
      "790000 33.195175445079805 mins\n",
      "800000 34.095977969964345 mins\n",
      "810000 35.00304854710897 mins\n",
      "820000 35.92482244968414 mins\n",
      "830000 36.848880191644035 mins\n",
      "840000 37.79589287439982 mins\n",
      "850000 38.7511078397433 mins\n",
      "860000 39.71569135983785 mins\n",
      "870000 40.69426135222117 mins\n",
      "880000 41.67988293965657 mins\n",
      "890000 42.682108255227405 mins\n",
      "900000 43.684333574771884 mins\n",
      "910000 44.71433179378509 mins\n",
      "920000 45.743629864851634 mins\n",
      "930000 46.78746453523636 mins\n",
      "940000 47.850286813577014 mins\n",
      "950000 48.92067739168803 mins\n",
      "960000 50.019057714939116 mins\n",
      "970000 51.12262223958969 mins\n",
      "980000 52.23357201019923 mins\n",
      "990000 53.35425728956859 mins\n",
      "1000000 54.481560957431796 mins\n",
      "1010000 55.62086708943049 mins\n",
      "1020000 56.77944422562917 mins\n",
      "1030000 57.95985959768295 mins\n",
      "finish converting\n",
      "[(80,   1,   0) (80,   2,   1) ( 7,  41,  40) ( 7,  81,  80)\n",
      " (57,  82,  81) (63, 672, 671) (63, 673, 672) (58, 712, 711)\n",
      " (57, 768, 767) (22, 833, 832) (22, 834, 833)]\n",
      "Wall time: 58min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if datatype<2:\n",
    "    file_info=EMdata.read_relion(file_path)\n",
    "    if datatype==0:\n",
    "        #read data (relion3.1)\n",
    "        dataset=file_info.getRdata_31()\n",
    "        optics=file_info.extractoptic()\n",
    "    else:\n",
    "        #read relion 3.0\n",
    "        dataset=file_info.getRdata()\n",
    "    metadata=dataset[0]\n",
    "    print(metadata)\n",
    "    data=dataset[1]\n",
    "    print(data[0])\n",
    "    #corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "    #label_path='F:/script/class2vec/real_star_file/self_unsupervised/10230_485_ctf/custom_single/pretext/'\n",
    "    #label=np.load(label_path+'/classes_KM.npy')\n",
    "    if fast:\n",
    "        corpus_information=EMdata.process_helical(dataset).extarct_helical_select_fast()\n",
    "    else:\n",
    "        corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "else:\n",
    "    #read cryosparc\n",
    "    dataset=np.load(file_path)\n",
    "    corpus_information=EMdata.process_cryosparc_helical(dataset).extract_helical()\n",
    "corpus_dic,helix_name=corpus_information\n",
    "if fast:\n",
    "    corpus=corpus_dic\n",
    "else:\n",
    "    corpus=list(corpus_dic.values())\n",
    "print(corpus[0])\n",
    "corpus_backup=corpus[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cryosparc\n",
    "#corpus_ignore=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ignore=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(str(particle[0]))\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(str(particle[0]))\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=['0']\n",
    "                count+=1               \n",
    "    corpus_ignore.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['80', '80', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '7', '57', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '63', '63', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '58', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '57', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '22', '22']\n"
     ]
    }
   ],
   "source": [
    "#corpus_ignore=corpus\n",
    "print(corpus_ignore[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2813\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_length_histogram=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    corpus_length_histogram.append(len(corpus_ignore[i]))\n",
    "plt.hist(corpus_length_histogram,list(range(0,max(corpus_length_histogram)+10,1)))\n",
    "#plt.ylim((0,1000))\n",
    "print(max(corpus_length_histogram))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus_ignore))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokens\\\\vocab.json', './tokens\\\\merges.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "if os.path.isdir(output_path+\"/corpus\") is False:\n",
    "    os.mkdir(output_path+\"/corpus\")\n",
    "paths = [str(x) for x in Path(output_path+\"/corpus/\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocabulary_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "if os.path.isdir(output_path+\"/tokens\") is False:\n",
    "    os.mkdir(output_path+\"/tokens\")\n",
    "if os.path.isdir(\"./tokens\") is False:\n",
    "    os.mkdir(\"./tokens\")\n",
    "tokenizer.save_model(output_path+\"/tokens\")\n",
    "tokenizer.save_model(\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(output_path+'/tokens/vocab.json') as f:\n",
    "    decode = json.load(f)\n",
    "encode={value:key for (key, value) in decode.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_code=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    lst=corpus_ignore[i]\n",
    "    corpus_row=[]\n",
    "    for j in range(len(lst)):\n",
    "        corpus_row.append(encode[word_to_index[lst[j]]+5])\n",
    "    corpus_code.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+\"/corpus/helical.txt\",\"w\") as f:\n",
    "    for i in range(len(corpus_code)):\n",
    "        lst=corpus_code[i]\n",
    "        for j in range(len(lst)):\n",
    "            if j==len(lst)-1:\n",
    "                f.write(lst[j]+'\\n')\n",
    "            else:\n",
    "                f.write(lst[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~\n",
      "delete the ignored group None\n"
     ]
    }
   ],
   "source": [
    "if '0' in [i for i in vocabulary]:\n",
    "    del decode[encode[word_to_index['0']+5]]\n",
    "    print('delete the ignored group',print(encode[word_to_index['0']+5]))\n",
    "with open('./tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)\n",
    "with open(output_path+'/tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file F:/script/class2vec/real_star_file/synAB_job089/tokens\\added_tokens.json. We won't load it.\n",
      "Didn't find file F:/script/class2vec/real_star_file/synAB_job089/tokens\\special_tokens_map.json. We won't load it.\n",
      "Didn't find file F:/script/class2vec/real_star_file/synAB_job089/tokens\\tokenizer_config.json. We won't load it.\n",
      "loading file F:/script/class2vec/real_star_file/synAB_job089/tokens\\vocab.json\n",
      "loading file F:/script/class2vec/real_star_file/synAB_job089/tokens\\merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(output_path+\"/tokens\", max_len=514)\n",
    "#tokenizer.encode(encode[word_to_index['0']+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=256,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    position_embedding_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Creating features from dataset file at F:/script/class2vec/real_star_file/synAB_job089/corpus/helical.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "data_import = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=output_path+\"/corpus/helical.txt\",\n",
    "    block_size=block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data_import\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 137364\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 51513\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 11.00 GiB total capacity; 8.67 GiB already allocated; 168.40 MiB free; 8.85 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1411\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1414\u001b[0m         )\n\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1649\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1651\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1653\u001b[0m                 if (\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2344\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2345\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2375\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2376\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2377\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2379\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1104\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m         )\n\u001b[0;32m   1108\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    859\u001b[0m         )\n\u001b[0;32m    860\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 )\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         )\n\u001b[0;32m    416\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         )\n\u001b[0;32m    345\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    254\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;31m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 11.00 GiB total capacity; 8.67 GiB already allocated; 168.40 MiB free; 8.85 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_path+\"/tokens/\")\n",
    "trainer.save_model(\"./tokens/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    'feature-extraction',model=\"./tokens\",tokenizer=\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(feature_extraction(encode[word_to_index['50']+5])))\n",
    "#print(np.squeeze(feature_extraction('DGG'))[0]-np.squeeze(feature_extraction('JJ'))[0])\n",
    "print(len(''.join(corpus_code[2])))\n",
    "print(len(corpus_ignore[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_corpus(corpus,cut_length):\n",
    "    cut_index=[]\n",
    "    new_corpus=[]\n",
    "    cut_length=cut_length\n",
    "    print(len(corpus))\n",
    "    for i in range(len(corpus)):\n",
    "        lst=corpus[i]\n",
    "        n=len(lst)\n",
    "        if n<=cut_length:\n",
    "            new_corpus.append(lst)\n",
    "            continue\n",
    "        if n%cut_length==0:\n",
    "            cut_amount=int(n/cut_length)\n",
    "        else:\n",
    "            cut_amount=int((n-n%cut_length)/cut_length)+1\n",
    "        for j in range(cut_amount-1):\n",
    "            cut_index.append(i)\n",
    "            new_corpus.append(lst[j*cut_length:(j+1)*cut_length])\n",
    "        new_corpus.append(lst[(cut_amount-1)*cut_length:])\n",
    "    print(len(new_corpus))\n",
    "    return new_corpus,cut_index\n",
    "corpus_code_cut,cut_index=cut_corpus(corpus_code,block_size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filament_embeddings=[]\n",
    "for i in range(len(corpus_code_cut)):\n",
    "    if i%200==0:\n",
    "        print(i)\n",
    "    lst=list(np.squeeze(feature_extraction(''.join(corpus_code_cut[i])))[0])\n",
    "    filament_embeddings.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans,SpectralClustering,MeanShift, estimate_bandwidth,AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filament_embeddings=np.array(filament_embeddings)\n",
    "mask_1 = np.isfinite(filament_embeddings).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filament_embeddings[mask_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sum = PCA(n_components=2).fit_transform(filament_embeddings)\n",
    "#cluster_pca = KMeans(n_clusters=3).fit_predict(pca_sum[0:len(corpus)])\n",
    "pca_sum_hD = PCA(n_components=30).fit_transform(filament_embeddings)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.scatter(pca_sum[:,0], pca_sum[:,1],alpha=0.6,color='blue')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_pca.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_neighbors=15\n",
    "min_dist=0.1\n",
    "#umap_ND=umap.UMAP(n_neighbors=200,min_dist=0.4,n_components=100).fit_transform(filament_embeddings)\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,metric='cosine')\n",
    "umap_2D = reducer.fit_transform(filament_embeddings)\n",
    "umap_ND=umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,n_components=100).fit_transform(filament_embeddings)\n",
    "print('finish umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "n_cluster = range(1,20)\n",
    "for n in n_cluster:\n",
    "    print(n)\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(umap_ND)\n",
    "    res.append(np.average(np.min(cdist(umap_ND, kmeans.cluster_centers_, 'euclidean'), axis=1)))\n",
    "        \n",
    "plt.plot(n_cluster, res)\n",
    "plt.title('elbow curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "n_cluster = range(2,8)\n",
    "for i in n_cluster:\n",
    "    print(i)\n",
    "    kmeans_model = KMeans(n_clusters=i).fit(umap_2D)\n",
    "    labels = kmeans_model.labels_\n",
    "    a=metrics.silhouette_score(umap_ND, labels, metric='euclidean')\n",
    "    res.append(a)\n",
    "\n",
    "plt.plot(n_cluster, res)\n",
    "print(res)\n",
    "plt.title('silhouette curve')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_silhouette.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filament_cluster_number=res.index(max(res[1:]))+2\n",
    "#filament_cluster_number=5\n",
    "#print(filament_cluster_number)\n",
    "#umap_predict=KMeans(n_clusters=filament_cluster_number).fit_predict(umap_ND)\n",
    "#umap_predict=SpectralClustering(n_clusters=filament_cluster_number).fit_predict(umap_ND)\n",
    "umap_predict=DBSCAN(eps=0.35, min_samples=400).fit_predict(umap_2D)+1\n",
    "filament_cluster_number=len(np.unique(umap_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "for i in range(filament_cluster_number):\n",
    "    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i+1)/filament_cluster_number), \n",
    "                                             label=str(i)+' : '+str(np.count_nonzero(umap_predict==i)))\n",
    "plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "#plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c=plt.cm.tab20((umap_predict)/filament_cluster_number))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.7,c=plt.cm.tab20((umap_predict+1)/filament_cluster_number),s=2)\n",
    "#plt.xlim((-5,10))\n",
    "#plt.ylim((-10,6))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_np.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "#c=plt.cm.tab20((umap_predict+1)/filament_cluster_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "#for i in range(filament_cluster_number):\n",
    "#    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i)/filament_cluster_number), \n",
    "#                                             label=str(i)+' : '+str(np.count_nonzero(umap_predict==i)))\n",
    "#plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "#plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c=plt.cm.tab20((umap_predict)/filament_cluster_number))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.5,c='blue')\n",
    "plt.xlim((-2,20))\n",
    "plt.ylim((-7,10))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_blue.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()\n",
    "#c=plt.cm.tab20((umap_predict+1)/filament_cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    cluster_nameID='clusterID'+str(i)\n",
    "    locals()[cluster_name]=[]\n",
    "    locals()[cluster_nameID]=[]\n",
    "\n",
    "cluster_choice=umap_predict\n",
    "positive_label=[]\n",
    "cut_index=np.array(cut_index)\n",
    "for i in range(len(helix_name)):\n",
    "    if i in cut_index:\n",
    "        t=np.count_nonzero(cut_index==i)\n",
    "        while t>0:\n",
    "            positive_label.append(helix_name[i][11:14])\n",
    "            t-=1\n",
    "    positive_label.append(helix_name[i][11:14])\n",
    "positive_label=np.array(positive_label)\n",
    "labels=list(np.unique(positive_label))\n",
    "positive_label_new=np.array([float(labels.index(x)) for x in positive_label])\n",
    "#labels_name=['type 3','type 1B','type 2B','type 1A','type 2A','type 2AB'] # define the type of filaments \n",
    "labels_name=['singlet','doublet']\n",
    "#labels_name=['1','2']\n",
    "#labels_name=['1','2','3','4','5']\n",
    "#labels_name=['SF','PHF']\n",
    "clustersize=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    clustersize.append(len(locals()['cluster'+str(i)]))\n",
    "print(positive_label_new[2])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(cut_index==126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(len(labels_name)):\n",
    "    locals()['true_label'+str(i)]=mpatches.Patch(color=plt.cm.tab20(i/3), label=labels_name[i])\n",
    "plt.legend(handles=[eval('true_label'+str(i)) for i in range(len(labels))])\n",
    "print(len(positive_label))\n",
    "#plt.xlim((2,17))\n",
    "#plt.ylim((0,15))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],color=plt.cm.tab20(positive_label_new/3),alpha=0.4)\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_label_bert.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive_label_new),len(cluster_choice))\n",
    "hist_data=pd.DataFrame({'labels':positive_label_new,'predict':cluster_choice})\n",
    "distribution_hist_all=[]\n",
    "print(hist_data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,filament_cluster_number):\n",
    "#    distribution=[]\n",
    "#    lst=hist_data[hist_data['predict']==i]\n",
    "#    for j in range(len(labels)):\n",
    "#        distribution.append(len(lst[lst['labels']==j])/len(lst))\n",
    "#    distribution_hist_all.append(distribution)\n",
    "#print(distribution_hist_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1,filament_cluster_number,figsize = (5*filament_cluster_number,7))\n",
    "#for i in range(filament_cluster_number):\n",
    "#    ax[i].bar(range(len(labels)),distribution_hist_all[i],tick_label =labels_name)\n",
    "#    particle_number=len(locals()['cluster'+str(i)])\n",
    "#    ax[i].set_title('cluster{} amount: {}'.format(i,particle_number))\n",
    "#\n",
    "#plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+'distr_new_bert.png')\n",
    "#plt.show()\n",
    "#print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill corpus\n",
    "corpus_fill=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(particle)\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(particle)\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=[(0,count,0)]\n",
    "                count+=1\n",
    "    corpus_fill.append(corpus_row)\n",
    "corpus_fill_cut,corpus_cut_index=cut_corpus(corpus_fill,block_size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    locals()['cluster'+str(i)]=[]\n",
    "    locals()['clusterID'+str(i)]=[]\n",
    "#count=0    \n",
    "for i in range(len(corpus_fill_cut)):\n",
    "    labels=umap_predict[i]\n",
    "    locals()['clusterID'+str(labels)].append(i)\n",
    "    lst=corpus_fill_cut[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if particle[0]!=0:\n",
    "            #count+=1\n",
    "            dataline=particle[-1]\n",
    "            locals()['cluster'+str(labels)].append(data[dataline])\n",
    "number=0\n",
    "for i in range(filament_cluster_number):\n",
    "    cluster_number_count=len(locals()['cluster'+str(i)])\n",
    "    print(i,cluster_number_count)\n",
    "    number=number+cluster_number_count\n",
    "print(number-len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input label\n",
    "#for i in range(filament_cluster_number):\n",
    "#    locals()['cluster'+str(i)]=[]\n",
    "#    locals()['clusterID'+str(i)]=[]\n",
    "#for i in range(len(corpus)):\n",
    "#    labels=umap_predict[i]\n",
    "#    locals()['clusterID'+str(labels)].append(i)\n",
    "#    lst=corpus[i]\n",
    "#    for j in range(len(lst)):\n",
    "#        dataline=i+j\n",
    "#        locals()['cluster'+str(labels)].append(data[dataline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    data_cluster=locals()[cluster_name]\n",
    "    if datatype==0:\n",
    "        output=EMdata.output_star(output_path+'/bert_'+file_name,i,data_cluster,metadata)\n",
    "        output.opticgroup(optics)\n",
    "        output.writecluster()\n",
    "    elif datatype==1:\n",
    "        output=EMdata.output_star(output_path+'/'+file_name,i,data_cluster,metadata)\n",
    "        output.writemetadata()\n",
    "        output.writecluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=list(np.squeeze(feature_extraction(''.join(corpus_code_cut[i])))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(''.join(corpus_code_cut[i]))\n",
    "print(inputs)\n",
    "lst2=model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(''.join(corpus_code_cut[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list=inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
