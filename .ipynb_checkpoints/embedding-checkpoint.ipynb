{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from numpy.random import multinomial\n",
    "import EMdata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import itertools\n",
    "import random, math\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU or CPU\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path\n",
    "file_path='F:/script/class2vec/real_star_file/10340_case2_chen_ig_T4.star'\n",
    "datatype=0 #0 is relion 3.1, 1 is relion 3, 2 is cryosparc\n",
    "\n",
    "file_name=os.path.basename(file_path)\n",
    "output_path=os.path.dirname(file_path)+'/'+os.path.splitext(file_name)[0]\n",
    "if os.path.isdir(output_path) is False:\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "#window size\n",
    "#w = 4\n",
    "\n",
    "#ignore classes\n",
    "#ignore_classes=[3,13,15,44,23,7,22,26,14,28,18,38,47,48,11,35,37]\n",
    "#ignore_classes=[1,2,3,5,8,10,14,15,17,18,19,21,22,23,24,27,28,31,32,33,34,35,37,38,40,41,44] #GSS20173_300\n",
    "#ignore_classes=[1,6,7,8,16,17,19,20,21,24,36,38,46] \n",
    "#ignore_classes=[14] #10243_tau\n",
    "#ignore_classes=[2,3,4,11,12,19,20,25,33,36,37,40,43,44,45,46,47,48,49,50] #10340_Case2_tau_ig\n",
    "#ignore_classes=[2,3,4,11,12,13,19,20,25,33,36,37,40,42,43,44,45,46,47,48,49,50] #10340_Case2_tau_ig by observe\n",
    "#ignore_classes=[2,5,6,7,12,16,19,23,24,25,30,31,32,33,39,44,45,50] #10340_Case2_tau_ig_E8\n",
    "#ignore_classes=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datatype<2:\n",
    "    file_info=EMdata.read_relion(file_path)\n",
    "    if datatype==0:\n",
    "        #read data (relion3.1)\n",
    "        dataset=file_info.getRdata_31()\n",
    "        optics=file_info.extractoptic()\n",
    "    else:\n",
    "        #read relion 3.0\n",
    "        dataset=file_info.getRdata()\n",
    "    metadata=dataset[0]\n",
    "    print(metadata)\n",
    "    data=dataset[1]\n",
    "    print(data[0])\n",
    "    corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "else:\n",
    "    #read cryosparc\n",
    "    dataset=np.load(file_path)\n",
    "    corpus_information=EMdata.process_cryosparc_helical(dataset).extract_helical()\n",
    "corpus_dic=corpus_information[0]\n",
    "corpus=list(corpus_dic.values())\n",
    "corpus_backup=corpus[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ignore=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(particle[0])\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(particle[0])\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=[0]\n",
    "                count+=1               \n",
    "    corpus_ignore.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_length_histogram=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    corpus_length_histogram.append(len(corpus_ignore[i]))\n",
    "fig,ax=plt.subplots(2)\n",
    "ax[0].hist(corpus_length_histogram,list(range(0,max(corpus_length_histogram)+10,10)))\n",
    "ax[1].bar(list(range(0,len(corpus_backup))),corpus_length_histogram)\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"length_histogram.png\",bbox_inches='tight', pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-mer seperation\n",
    "def overlaping_kmer(data,k):\n",
    "    kmer_corpus=[]\n",
    "    for i in range(len(data)):\n",
    "        lst=data[i]\n",
    "        kmer_lst=[]\n",
    "        for j in range(len(lst)-k+1):\n",
    "            kmer_lst.append(str(lst[j])+'-'+str(lst[j+1]))\n",
    "        kmer_corpus.append(kmer_lst)\n",
    "    return kmer_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_corpus(corpus,cut_length):\n",
    "    new_corpus=[]\n",
    "    cut_length=cut_length\n",
    "    print(len(corpus))\n",
    "    for i in range(len(corpus)):\n",
    "        lst=corpus[i]\n",
    "        n=len(lst)\n",
    "        if n<cut_length:\n",
    "            new_corpus.append(lst)\n",
    "            continue\n",
    "        cut_amount=int((n-n%cut_length)/cut_length)\n",
    "        for j in range(cut_amount-1):\n",
    "            new_corpus.append(lst[j*cut_length:(j+1)*cut_length])\n",
    "        new_corpus.append(lst[(cut_amount-1)*cut_length:])\n",
    "    print(len(new_corpus))\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus=overlaping_kmer(corpus_backup,2)\n",
    "#corpus_context=corpus[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus_ignore))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus=cut_corpus(corpus_backup,60)\n",
    "#corpus_context=corpus_backup[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative(sample_size):\n",
    "    sample_probability = {}\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus_ignore))))\n",
    "    normalizing = sum([v**0.75 for v in word_counts.values()])\n",
    "    for word in word_counts:\n",
    "        sample_probability[word] = word_counts[word]**0.75 / normalizing\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                 word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to windoes vector\n",
    "w = 1\n",
    "context_tuple_list = []\n",
    "negative_samples = sample_negative(4)\n",
    "\n",
    "for text in corpus_ignore:\n",
    "    for i, word in enumerate(text):\n",
    "        if word==0:\n",
    "            continue\n",
    "        first_context_word_index = max(0,i-w)\n",
    "        last_context_word_index = min(i+w, len(text))\n",
    "        for j in range(first_context_word_index, last_context_word_index):\n",
    "            if word==0:\n",
    "                continue\n",
    "            if i!=j:\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} pairs of target and context words\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        batch_target.append(word_to_index[context_tuple_list[i][0]])\n",
    "        batch_context.append(word_to_index[context_tuple_list[i][1]])\n",
    "        batch_negative.append([word_to_index[w] for w in context_tuple_list[i][2]])\n",
    "        if (i+1) % batch_size == 0 or i == len(context_tuple_list)-1:\n",
    "            tensor_target = torch.from_numpy(np.array(batch_target)).long().to(device)\n",
    "            tensor_context = torch.from_numpy(np.array(batch_context)).long().to(device)\n",
    "            tensor_negative = torch.from_numpy(np.array(batch_negative)).long().to(device)\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target = nn.Embedding(vocab_size, embedding_size).cuda()\n",
    "        self.context = nn.Embedding(vocab_size, embedding_size).cuda()\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        emb_target = self.target(target_word)\n",
    "        emb_context = self.context(context_word)\n",
    "        emb_product = torch.mul(emb_target, emb_context).cuda()\n",
    "        emb_product = torch.sum(emb_product, dim=1).cuda()\n",
    "        out = torch.sum(F.logsigmoid(emb_product)).cuda()\n",
    "        emb_negative = self.context(negative_example)\n",
    "        emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2)).cuda()\n",
    "        emb_product = torch.sum(emb_product, dim=1).cuda()\n",
    "        out += torch.sum(F.logsigmoid(-emb_product)).cuda()\n",
    "        return -out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.1):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(gain*100))\n",
    "        if gain < self.min_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "net = Word2Vec(embedding_size=embedding_size, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "n=0\n",
    "while True:\n",
    "    n=n+1\n",
    "    print(n)\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=1000)\n",
    "    for i in range(len(context_tuple_batches)):\n",
    "        net.zero_grad()\n",
    "        target_tensor, context_tensor, negative_tensor = context_tuple_batches[i]\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "    print(\"Loss: \", torch.mean(torch.stack(losses)),torch.stack(losses)[0],torch.stack(losses)[-1])\n",
    "    early_stopping.update_loss(torch.mean(torch.stack(losses)))\n",
    "    if early_stopping.stop() is True:\n",
    "        break\n",
    "    if torch.mean(torch.stack(losses))<10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_tuple_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show 2D class embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans,SpectralClustering,MeanShift, estimate_bandwidth,AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS = net.target.weight.data.cpu().numpy()\n",
    "print('EMBEDDINGS.shape: ', EMBEDDINGS.shape)\n",
    "\n",
    "res = []\n",
    "n_cluster = range(1,6)\n",
    "for n in n_cluster:\n",
    "    print(n)\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(EMBEDDINGS)\n",
    "    res.append(np.average(np.min(cdist(EMBEDDINGS, kmeans.cluster_centers_, 'euclidean'), axis=1)))\n",
    "        \n",
    "plt.plot(n_cluster, res)\n",
    "plt.title('elbow curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the 2D class segments embedding\n",
    "class_umap=umap.UMAP(n_neighbors=50,min_dist=0,n_components=2).fit_transform(EMBEDDINGS)\n",
    "cluster_number=6\n",
    "kmeans_best = SpectralClustering(n_clusters=cluster_number).fit_predict(class_umap)\n",
    "EMBEDDINGS_np=np.array(EMBEDDINGS)\n",
    "plt.figure(figsize = (5,5))\n",
    "for i in range(vocabulary_size):\n",
    "    # print('vocab_idx: ', vocab_idx)\n",
    "    plt.scatter(class_umap[i][0], class_umap[i][1],color=plt.cm.RdYlBu(kmeans_best[i]/3))\n",
    "    plt.annotate(index_to_word[i], xy = (class_umap[i][0], class_umap[i][1]), \\\n",
    "        ha='right',va='bottom')\n",
    "plt.savefig(output_path+'/'+'w2v.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#word_to_cluster={}\n",
    "#for i in range(cluster_number):\n",
    "#    word_to_cluster[str(i)]= np.array(np.where(kmeans_best==i)).ravel()+1\n",
    "#print(word_to_cluster)\n",
    "\n",
    "##using the histogram method to cluster \n",
    "\n",
    "\n",
    "\n",
    "#cluster_corpus=corpus[:]\n",
    "#for i in range(cluster_number):\n",
    "#    cluster_lst=list(word_to_cluster[str(i)])\n",
    "#    for j in range(len(cluster_corpus)):\n",
    "#        filament=cluster_corpus[j]\n",
    "#        filament=[i if np.int64(x) in cluster_lst else x for x in filament]\n",
    "#        cluster_corpus[j]=filament\n",
    "\n",
    "#filament_cluster_percentage=[]\n",
    "#for filament in cluster_corpus:\n",
    "#    filament_lenghth=len(filament)\n",
    "#    pert=[]\n",
    "#    for i in range(cluster_number):\n",
    "#        cluster_per=float(filament.count(i))/filament_lenghth\n",
    "#        pert.append(cluster_per)\n",
    "#    filament_cluster_percentage.append(pert)\n",
    "\n",
    "#histogram clustering\n",
    "#pca_filaments = PCA(n_components=2).fit_transform(filament_cluster_percentage)\n",
    "#plt.figure(figsize = (5, 5))\n",
    "#for i in range(len(cluster_corpus)):\n",
    "#    plt.scatter(pca_filaments[i][0], pca_filaments[i][1],color='blue')\n",
    "#plt.savefig(\"cluster.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Filament Embedding and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average filament embedding\n",
    "from scipy import stats\n",
    "filament_cluster_number=2\n",
    "average_method=0 # 0 is average, 1 is weight average\n",
    "filament_score=[]\n",
    "all_data=[]\n",
    "for filament in corpus_ignore:\n",
    "    score=torch.zeros(embedding_size)\n",
    "    counts=0\n",
    "    filament_list=[]\n",
    "    for i in filament:\n",
    "        if i==0:\n",
    "            continue\n",
    "        counts+=1\n",
    "        filament_list.append(EMBEDDINGS[word_to_index[i]])\n",
    "    filament_list=np.array(filament_list)\n",
    "    mean=filament_list.mean(axis=0)\n",
    "    if average_method==0:\n",
    "        filament_score.append(mean)\n",
    "    elif average_method==1:\n",
    "        dim=len(filament_list[0])\n",
    "        filament_normalized=np.exp(-0.5*((filament_list-mean) @ (filament_list-mean).T*0)).diagonal()/np.sqrt(np.pi**dim*0.05)\n",
    "        filament_normalized=filament_normalized/filament_normalized.sum()\n",
    "        score=filament_normalized @ filament_list\n",
    "        if counts<=2:\n",
    "            continue\n",
    "        filament_score.append(np.array(score))\n",
    "all_data=filament_score[:]\n",
    "all_data.extend(EMBEDDINGS_np)\n",
    "filament_number=len(filament_score)\n",
    "print(filament_number)\n",
    "#filament_normalized=np.exp(-0.5*((filament_list-mean) @ (filament_list-mean).T)).diagonal()/np.sqrt(np.pi**dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_sum = PCA(n_components=2).fit_transform(all_data)\n",
    "pca_sum_3D = PCA(n_components=3).fit_transform(all_data)\n",
    "#cluster_pca = KMeans(n_clusters=3).fit_predict(pca_sum[0:len(corpus)])\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.scatter(pca_sum[:,0], pca_sum[:,1],alpha=0.6,color='blue')\n",
    "for i in range(len(EMBEDDINGS_np)):\n",
    "    plt.scatter(pca_sum[i+filament_number][0], pca_sum[i+filament_number][1],color='black',marker='*')\n",
    "    plt.annotate(index_to_word[i], xy = (pca_sum[i+filament_number][0], pca_sum[i+filament_number][1]), ha='right',va='bottom')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_pca.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()\n",
    "# color=plt.cm.RdYlBu(pca_Clustering[i]/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umap of the filament embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=20\n",
    "min_dist=0.1\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist)\n",
    "umap_2D = reducer.fit_transform(all_data)\n",
    "umap_3D = umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,n_components=3).fit_transform(pca_sum_3D)\n",
    "#filament_umap_ND=umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,n_components=50).fit_transform(all_data)[0:filament_number]\n",
    "filament_umap=umap_2D[0:filament_number]\n",
    "filament_umap_3D=umap_3D[0:filament_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+'/'+'umap_3D_normal.npy', 'wb') as f:\n",
    "    np.save(f, filament_umap_3D)\n",
    "print('finish umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filament_cluster_number=2\n",
    "#umap_predict=SpectralClustering(n_clusters=filament_cluster_number).fit_predict(filament_umap_3D)\n",
    "#cluster_umap =  KMeans(n_clusters=filament_cluster_number).fit(umap[0:filament_number])\n",
    "#cluster_umap = AgglomerativeClustering(n_clusters=filament_cluster_number,distance_threshold=None).fit(umap[0:filament_number])\n",
    "#umap_predict=cluster_umap.fit_predict(filament_umap)\n",
    "\n",
    "\n",
    "umap_predict=DBSCAN(eps=0.5, min_samples=120).fit_predict(filament_umap)+1\n",
    "filament_cluster_number=len(np.unique(umap_predict))\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(filament_cluster_number):\n",
    "    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i+1)/filament_cluster_number), label=str(i))\n",
    "plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "plt.scatter(filament_umap[:,0], filament_umap[:,1],alpha=0.6,color=plt.cm.tab20((umap_predict+1)/filament_cluster_number))\n",
    "for i in range(len(EMBEDDINGS_np)):\n",
    "    plt.scatter(umap_2D[i+filament_number][0], umap_2D[i+filament_number][1],color='black',marker='*',s=50)\n",
    "    plt.annotate(index_to_word[i], xy = (umap_2D[i+filament_number][0], umap_2D[i+filament_number][1]), ha='right',va='bottom')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_umap.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()\n",
    "# color=plt.cm.tab20((umap_predict+1)/filament_cluster_number)\n",
    "# color='blue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    cluster_nameID='clusterID'+str(i)\n",
    "    locals()[cluster_name]=[]\n",
    "    locals()[cluster_nameID]=[]\n",
    "data_line=0\n",
    "t=0\n",
    "cluster_choice=umap_predict\n",
    "positive_label=[]\n",
    "for i in range(len(corpus)):\n",
    "    lst=corpus[i]\n",
    "    cluster_number=cluster_choice[i]\n",
    "    cluster_name='cluster'+str(cluster_number)\n",
    "    cluster_nameID='clusterID'+str(cluster_number)\n",
    "    #print(cluster_name,i)\n",
    "    for j in range(len(lst)):\n",
    "        locals()[cluster_name].append(data[data_line])\n",
    "        locals()[cluster_nameID].append(data[data_line][8][18:21])\n",
    "        #locals()[cluster_nameID].append(data[data_line][0][67:70])\n",
    "        data_line+=1\n",
    "    positive_label.append(locals()[cluster_nameID][-1])\n",
    "positive_label=np.array(positive_label)\n",
    "labels=list(np.unique(positive_label))\n",
    "positive_label_new=np.array([float(labels.index(x)) for x in positive_label])\n",
    "labels_name=['singlet','doublet'] # define the type of filaments \n",
    "clustersize=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    clustersize.append(len(locals()['cluster'+str(i)]))\n",
    "print(clustersize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with the positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram on rate of successful\n",
    "#labels=['038','029'] #tau 10230\n",
    "#labels_name=['PHF','SF']\n",
    "labels=['024','025'] #tau 10340\n",
    "labels_name=['singlet','doublet']\n",
    "distribution_hist_all=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    distribution_hist=[]\n",
    "    lst=locals()['clusterID'+str(i)]\n",
    "    for j in range(filament_cluster_number):\n",
    "        group_percentage=lst.count(labels[j])/len(lst)\n",
    "        distribution_hist.append(group_percentage)\n",
    "    distribution_hist_all.append(distribution_hist)\n",
    "print(distribution_hist_all)\n",
    "print(len(data),len(cluster0),len(cluster1))\n",
    "\n",
    "fig, ax = plt.subplots(1,filament_cluster_number)\n",
    "\n",
    "for i in range(filament_cluster_number):\n",
    "    ax[i].bar(range(filament_cluster_number),distribution_hist_all[i],tick_label =labels_name)\n",
    "    particle_number=len(locals()['cluster'+str(i)])\n",
    "    ax[i].set_title('cluster{} amount: {}'.format(i,particle_number))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+'distr_new.png')\n",
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][7][18:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(len(labels_name)):\n",
    "    locals()['true_label'+str(i)]=mpatches.Patch(color=plt.cm.tab20(i/3), label=labels_name[i])\n",
    "plt.legend(handles=[eval('true_label'+str(i)) for i in range(len(labels))])\n",
    "print(len(positive_label))\n",
    "plt.scatter(filament_umap[:,0], filament_umap[:,1],color=plt.cm.tab20(positive_label_new/3),alpha=0.4)\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_umap_label.png\",bbox_inches='tight', pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label distribution\n",
    "#from sklearn.manifold import TSNE, MDS,Isomap,SpectralEmbedding\n",
    "#tsne =SpectralEmbedding(n_components=2,n_neighbors=10).fit_transform(all_data)\n",
    "#filament_try=tsne[0:filament_number]\n",
    "\n",
    "positive_label_new=np.array([float(labels.index(x)) for x in positive_label])\n",
    "with open(output_path+'/'+'positive_label.npy', 'wb') as f:\n",
    "    np.save(f, filament_umap_3D)\n",
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(filament_cluster_number):\n",
    "    locals()['true_label'+str(i)]=mpatches.Patch(color=plt.cm.tab20(i/(len(labels)+1)), label=labels_name[i])\n",
    "plt.legend(handles=[eval('true_label'+str(i)) for i in range(len(labels))])\n",
    "print(len(positive_label))\n",
    "plt.scatter(filament_umap[:,0], filament_umap[:,1],color=plt.cm.tab20(positive_label_new/3),alpha=1)\n",
    "for i in range(len(EMBEDDINGS_np)):\n",
    "    plt.scatter(umap_2D[i+filament_number][0], umap_2D[i+filament_number][1],color='black',marker='*',s=50)\n",
    "    plt.annotate(index_to_word[i], xy = (umap_2D[i+filament_number][0], umap_2D[i+filament_number][1]), ha='right',va='bottom')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_umap_labels.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperarte subcluster\n",
    "for i in range(filament_cluster_number):\n",
    "    cluster=locals()['cluster'+str(i)]\n",
    "    lst=locals()['clusterID'+str(i)]\n",
    "    for j in range(filament_cluster_number):\n",
    "        subcluster_name='cluster'+str(i)+'_'+str(j)\n",
    "        locals()[subcluster_name]=[x for x, y in zip(cluster,lst) if y==labels[j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write .star file for the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    data_cluster=locals()[cluster_name]\n",
    "    if datatype==0:\n",
    "        output=EMdata.output_star(output_path+'/'+file_name,i,data_cluster,metadata)\n",
    "        output.opticgroup(optics)\n",
    "        output.writecluster()\n",
    "    elif datatype==1:\n",
    "        output=EMdata.output_star(output_path+'/'+file_name,i,data_cluster,metadata)\n",
    "        output.writemetadata()\n",
    "        output.writecluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write subcluster relion\n",
    "for i in range(filament_cluster_number):\n",
    "    for j in range(filament_cluster_number):\n",
    "        cluster_name='cluster'+str(i)+'_'+str(j)\n",
    "        data_cluster=locals()[cluster_name]\n",
    "        if datatype==0:\n",
    "            output=EMdata.output_star(output_path+'/'+file_name,str(i)+'_'+str(j),data_cluster,metadata)\n",
    "            output.opticgroup(optics)\n",
    "            output.writecluster()\n",
    "        elif datatype==1:\n",
    "            output=EMdata.output_star(output_path+'/'+file_name,str(i)+'_'+str(j),data_cluster,metadata)\n",
    "            output.writemetadata()\n",
    "            output.writecluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
