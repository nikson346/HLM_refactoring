{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import EMdata\n",
    "import torch\n",
    "import itertools\n",
    "import random, math\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU or CPU\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path\n",
    "file_path='F:/script/class2vec/real_star_file/10340_case2_chen_ig_T4_select.star'\n",
    "datatype=0 #0 is relion 3.1, 1 is relion 3, 2 is cryosparc\n",
    "block_size=64\n",
    "\n",
    "file_name=os.path.basename(file_path)\n",
    "output_path=os.path.dirname(file_path)+'/'+os.path.splitext(file_name)[0]\n",
    "if os.path.isdir(output_path) is False:\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_rlnCoordinateX', '_rlnCoordinateY', '_rlnHelicalTubeID', '_rlnAngleTiltPrior', '_rlnAnglePsiPrior', '_rlnHelicalTrackLengthAngst', '_rlnAnglePsiFlipRatio', '_rlnAngleRotFlipRatio', '_rlnImageName', '_rlnMicrographName', '_rlnOpticsGroup', '_rlnCtfMaxResolution', '_rlnCtfFigureOfMerit', '_rlnDefocusU', '_rlnDefocusV', '_rlnDefocusAngle', '_rlnCtfBfactor', '_rlnCtfScalefactor', '_rlnPhaseShift', '_rlnGroupNumber', '_rlnAngleRot', '_rlnAngleTilt', '_rlnAnglePsi', '_rlnOriginXAngst', '_rlnOriginYAngst', '_rlnClassNumber', '_rlnNormCorrection', '_rlnLogLikeliContribution', '_rlnMaxValueProbDistribution', '_rlnNrOfSignificantSamples']\n",
      "['898.487833', '3433.234052', '1', '90.000000', '14.338115', '983.250000', '0.500000', '0.500000', '000037@Extract/job025/Micrographs/Case2/FoilHole_24943301_Data_24944186_24944187_20190125_1838-129196.mrcs', 'Micrographs/Case2/FoilHole_24943301_Data_24944186_24944187_20190125_1838-129196.mrc', '1', '4.077273', '0.263951', '23564.167969', '23224.824219', '-59.50628', '0.000000', '1.000000', '0.000000', '5', '0.000000', '90.000000', '14.984124', '-3.00036', '-3.02953', '28', '0.767223', '1.668897e+05', '0.922885', '9']\n",
      "finish reading\n",
      "finish converting\n",
      "[(57,  1,    99) (57,  2,   100) (45,  3,     6) (57,  4,   101)\n",
      " (57,  5,   102) (57,  6,   103) (45,  7,     7) (45,  8,     8)\n",
      " (45,  9,     9) (45, 10,    10) (45, 11,    11) (38, 12,    12)\n",
      " (38, 13,    13) (45, 14,    12) (38, 15,    14) (38, 16,    15)\n",
      " (34, 17,  4077) (55, 18,   581) (55, 19,   582) (55, 20,   583)\n",
      " (34, 21,  4078) (34, 22,  4079) (34, 23,  4080) (34, 24,  4081)\n",
      " (37, 25, 34933) (34, 26,  4082) (67, 27,     3) (67, 28,     4)\n",
      " (67, 29,     5) (67, 30,     6) (40, 31,  1423) (40, 32,  1424)\n",
      " (40, 33,  1425) (40, 34,  1426) (40, 35,  1427) (40, 36,  1428)\n",
      " (28, 37,     0) (28, 38,     1) (28, 39,     2) (28, 40,     3)\n",
      " (28, 41,     4) (28, 42,     5) (28, 43,     6) (28, 44,     7)\n",
      " (13, 45,   434) (13, 46,   435) (26, 47,     1) (66, 48,    16)\n",
      " (66, 49,    17) (58, 50,     1) (58, 51,     2) (58, 52,     3)\n",
      " (58, 53,     4) (58, 54,     5) (58, 55,     6) (54, 56, 16110)\n",
      " (54, 57, 16111) (54, 58, 16112) (54, 59, 16113) (54, 60, 16114)]\n",
      "[(47,   1,  1458) (47,   2,  1459) (18,   3, 43931) (18,   4, 43932)\n",
      " (18,   5, 43933) (18,   6, 43934) (13,   7,   436) ( 1,   8,   181)\n",
      " ( 1,   9,   182) ( 1,  10,   183) ( 1,  11,   184) ( 1,  12,   185)\n",
      " (18,  13, 43935) (24,  14,     5) (24,  15,     6) (24,  16,     7)\n",
      " (32,  17,   266) (32,  18,   267) (32,  19,   268) (32,  20,   269)\n",
      " (58,  21,     7) (58,  22,     8) (58,  23,     9) (44,  24,    18)\n",
      " (44,  25,    19) (58,  26,    10) (58,  27,    11) (58,  28,    12)\n",
      " (58,  29,    13) (58,  30,    14) ( 3,  31,    79) (58,  32,    15)\n",
      " (58,  33,    16) (58,  34,    17) (58,  35,    18) ( 3,  36,    80)\n",
      " ( 3,  37,    81) ( 3,  38,    82) ( 3,  39,    83) ( 3,  40,    84)\n",
      " ( 4,  41,  1611) ( 4,  42,  1612) ( 4,  43,  1613) ( 4,  44,  1614)\n",
      " ( 4,  45,  1615) ( 4,  46,  1616) (53,  47,    11) (53,  48,    12)\n",
      " (53,  49,    13) (53,  50,    14) (51,  51,     6) (53,  52,    15)\n",
      " (50,  53,  2976) (50,  54,  2977) (29,  55,     2) (46,  56,  1326)\n",
      " (28,  57,     0) (60,  58,    42) ( 4,  59,  1617) (50,  60,  2978)\n",
      " ( 4,  61,  1618) (22,  62,  8759) (22,  63,  8760) (22,  64,  8761)\n",
      " ( 7,  65,  6696) ( 7,  66,  6697) ( 7,  67,  6698) ( 7,  68,  6699)\n",
      " ( 7,  69,  6700) ( 7,  70,  6701) ( 7,  71,  6702) ( 7,  72,  6703)\n",
      " ( 7,  73,  6704) ( 7,  74,  6705) ( 7,  75,  6706) (59,  76, 12367)\n",
      " (27,  77,     9) (27,  78,    10) (27,  79,    11) (27,  80,    12)\n",
      " (27,  81,    13) (27,  82,    14) (27,  83,    15) (27,  84,    16)\n",
      " (26,  85,     2) (26,  86,     3) (26,  87,     4) (27,  88,    17)\n",
      " (26,  89,     5) (26,  90,     6) (26,  91,     7) (26,  92,     8)\n",
      " (26,  93,     9) (26,  94,    10) (26,  95,    11) (26,  96,    12)\n",
      " (26,  97,    13) ( 4,  98,  1619) (26,  99,    14) ( 3, 100,    85)\n",
      " (52, 101,     9) ( 4, 102,  1620) ( 4, 103,  1621) (52, 104,    10)\n",
      " (52, 105,    11) (52, 106,    12) (52, 107,    13) ( 4, 108,  1622)\n",
      " ( 4, 109,  1623) ( 4, 110,  1624) (54, 111, 16115) (54, 112, 16116)\n",
      " (54, 113, 16117)]\n",
      "[(17, 114,  1424) (17, 116,  1425) (17, 117,  1426) (17, 118,  1427)\n",
      " (17, 119,  1428) ( 5, 120, 30328) (17, 121,  1429) (17, 122,  1430)\n",
      " (17, 123,  1431) (17, 124,  1432) (17, 125,  1433) (17, 126,  1434)\n",
      " (17, 127,  1435) (17, 128,  1436) (20, 129, 18426) (12, 130,   924)\n",
      " (12, 131,   925) (12, 132,   926) (20, 133, 18427) (20, 134, 18428)\n",
      " (20, 135, 18429) (20, 137, 18430) (18, 138, 43936) (18, 139, 43937)\n",
      " (18, 140, 43938) (18, 141, 43939) (18, 142, 43940) (18, 143, 43941)\n",
      " (13, 144,   437) (66, 145,    18) (62, 146,  6335) (62, 147,  6336)\n",
      " (62, 148,  6337) (62, 149,  6338) (62, 150,  6339) (19, 151,  4666)\n",
      " (62, 152,  6340) (62, 153,  6341) (62, 154,  6342) (66, 155,    19)\n",
      " (44, 156,    20) (66, 157,    20) (32, 158,   270) (32, 159,   271)\n",
      " (32, 160,   272) (58, 161,    19) (58, 162,    20) (30, 163,  2133)\n",
      " (28, 164,     0) (30, 165,  2134) (13, 166,   438) (30, 167,  2135)\n",
      " (58, 168,    21) (58, 169,    22) (58, 170,    23) (30, 171,  2136)\n",
      " (28, 172,     1) (28, 173,     2) (28, 174,     3) (30, 175,  2137)\n",
      " (30, 176,  2138) (54, 177, 16118) (54, 178, 16119) ( 9, 179,    47)\n",
      " ( 9, 180,    48) (53, 181,    16) (53, 182,    17) (32, 183,   273)\n",
      " ( 9, 184,    49) (30, 185,  2139) (50, 186,  2979) (60, 187,    43)\n",
      " (50, 188,  2980) (50, 189,  2981) (18, 190, 43942) (18, 191, 43943)\n",
      " (18, 192, 43944) (18, 193, 43945) (18, 194, 43946) (28, 195,     4)\n",
      " (58, 196,    24) (58, 197,    25) (46, 198,  1327) (69, 199, 10474)]\n",
      "[(18,  5, 43948) (18,  6, 43949) (18,  7, 43950) (58,  8,    26)\n",
      " (45,  9,    14) (18, 10, 43951) (18, 11, 43952) (18, 12, 43953)\n",
      " (18, 13, 43954) (18, 14, 43955) (11, 15,     3) (10, 16, 16301)\n",
      " (10, 17, 16302) (30, 18,  2140) (28, 19,     0) ( 1, 20,   186)\n",
      " (32, 21,   274) (30, 22,  2141) (62, 23,  6343) (60, 24,    47)\n",
      " (32, 25,   275) (32, 26,   276) (32, 27,   277) (19, 28,  4667)\n",
      " (29, 29,     6) (55, 30,   584) (66, 31,     1) (38, 32,    16)\n",
      " (66, 33,     2) ( 1, 34,   187) (66, 35,     3) (66, 36,     4)\n",
      " (66, 37,     5) (48, 38,  1363) (18, 39, 43956) (18, 40, 43957)\n",
      " (66, 41,     6) (66, 42,     7) (48, 43,  1364) (48, 44,  1365)\n",
      " (32, 45,   278) (60, 46,    48) (54, 47, 16123) (16, 48, 11474)\n",
      " (32, 49,   279) (30, 50,  2142) (24, 51,     8) (24, 52,     9)\n",
      " (24, 53,    10) (24, 54,    11) ( 3, 55,    86) (54, 56, 16124)\n",
      " (54, 57, 16125) (44, 58,    21) (66, 59,     8) ( 9, 60,    52)\n",
      " ( 9, 61,    53) (32, 62,   280) (44, 63,    22) ( 3, 64,    87)\n",
      " (44, 65,    23) (24, 66,    12) (44, 67,    24) (24, 68,    13)\n",
      " (27, 69,    18) (24, 70,    14) (27, 71,    19) (27, 72,    20)\n",
      " (27, 73,    21) (52, 74,    15) (27, 75,    22) (44, 76,    25)\n",
      " (58, 77,    27) (58, 78,    28) (58, 79,    29) (51, 80,     9)\n",
      " (51, 81,    10) (32, 82,   281) (58, 83,    30) ( 3, 84,    88)\n",
      " ( 3, 85,    89) (58, 86,    31)]\n",
      "[(24,  92,   15) (65,  93, 3231) ( 4,  94, 1626) (61,  95, 4946)\n",
      " (51,  96,   11) (14,  97, 2010) (22,  98, 8764) (27,  99,   23)\n",
      " (14, 100, 2011) (44, 101,   26) (26, 102,   16) (27, 103,   24)\n",
      " (26, 104,   17) (26, 105,   18) (26, 106,   19) (26, 107,   20)\n",
      " (26, 108,   21) (11, 109,    4) (11, 110,    5) (44, 111,   27)\n",
      " (44, 112,   28) (44, 113,   29) (44, 114,   30) (44, 115,   31)\n",
      " (44, 116,   32) (44, 117,   33) (50, 118, 2983) (24, 119,   16)\n",
      " (24, 120,   17) (27, 121,   25) (24, 122,   18) (27, 123,   26)\n",
      " (58, 124,   32) (58, 125,   33) (58, 126,   34) (58, 127,   35)\n",
      " (58, 128,   36) (58, 129,   37) ( 4, 130, 1627) ( 4, 131, 1628)\n",
      " (52, 132,   16) (52, 133,   17) (52, 134,   18) ( 3, 135,   90)\n",
      " (44, 136,   34) ( 3, 137,   91) (44, 138,   35) (44, 139,   36)\n",
      " (44, 140,   37) (44, 141,   38) (28, 142,    0) (53, 143,   23)\n",
      " (24, 144,   19) (24, 145,   20) (24, 146,   21) (24, 147,   22)\n",
      " (11, 148,    6) (11, 149,    7) (11, 150,    8) (27, 151,   27)\n",
      " (11, 152,    9) (11, 153,   10) (11, 154,   11)]\n"
     ]
    }
   ],
   "source": [
    "if datatype<2:\n",
    "    file_info=EMdata.read_relion(file_path)\n",
    "    if datatype==0:\n",
    "        #read data (relion3.1)\n",
    "        dataset=file_info.getRdata_31()\n",
    "        optics=file_info.extractoptic()\n",
    "    else:\n",
    "        #read relion 3.0\n",
    "        dataset=file_info.getRdata()\n",
    "    metadata=dataset[0]\n",
    "    print(metadata)\n",
    "    data=dataset[1]\n",
    "    print(data[0])\n",
    "    corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "else:\n",
    "    #read cryosparc\n",
    "    dataset=np.load(file_path)\n",
    "    corpus_information=EMdata.process_cryosparc_helical(dataset).extract_helical()\n",
    "corpus_dic=corpus_information[0]\n",
    "corpus=list(corpus_dic.values())\n",
    "corpus_backup=corpus[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cryosparc\n",
    "#corpus_ignore=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ignore=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(str(particle[0]))\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(str(particle[0]))\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=['0']\n",
    "                count+=1               \n",
    "    corpus_ignore.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_length_histogram=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    corpus_length_histogram.append(len(corpus_ignore[i]))\n",
    "plt.hist(corpus_length_histogram,list(range(0,max(corpus_length_histogram)+10,10)))\n",
    "#plt.ylim((0,1000))\n",
    "print(max(corpus_length_histogram))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus_ignore))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "if os.path.isdir(output_path+\"/corpus\") is False:\n",
    "    os.mkdir(output_path+\"/corpus\")\n",
    "paths = [str(x) for x in Path(output_path+\"/corpus/\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocabulary_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "if os.path.isdir(output_path+\"/tokens\") is False:\n",
    "    os.mkdir(output_path+\"/tokens\")\n",
    "if os.path.isdir(\"./tokens\") is False:\n",
    "    os.mkdir(\"./tokens\")\n",
    "tokenizer.save_model(output_path+\"/tokens\")\n",
    "tokenizer.save_model(\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(output_path+'/tokens/vocab.json') as f:\n",
    "    decode = json.load(f)\n",
    "encode={value:key for (key, value) in decode.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_code=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    lst=corpus_ignore[i]\n",
    "    corpus_row=[]\n",
    "    for j in range(len(lst)):\n",
    "        corpus_row.append(encode[word_to_index[lst[j]]+5])\n",
    "    corpus_code.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+\"/corpus/helical.txt\",\"w\") as f:\n",
    "    for i in range(len(corpus_code)):\n",
    "        lst=corpus_code[i]\n",
    "        for j in range(len(lst)):\n",
    "            if j==len(lst)-1:\n",
    "                f.write(lst[j]+'\\n')\n",
    "            else:\n",
    "                f.write(lst[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '0' in [i for i in vocabulary]:\n",
    "    del decode[encode[word_to_index['0']+5]]\n",
    "    print('delete the ignored group',print(encode[word_to_index['0']+5]))\n",
    "with open('./tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)\n",
    "with open(output_path+'/tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(output_path+\"/tokens\", max_len=514)\n",
    "#tokenizer.encode(encode[word_to_index['0']+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=128,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "data_import = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=output_path+\"/corpus/helical.txt\",\n",
    "    block_size=block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_import[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data_import,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_path+\"/tokens/\")\n",
    "trainer.save_model(\"./tokens/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    'feature-extraction',model=\"./tokens\",tokenizer=\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(feature_extraction(encode[word_to_index['50']+5])))\n",
    "#print(np.squeeze(feature_extraction('DGG'))[0]-np.squeeze(feature_extraction('JJ'))[0])\n",
    "print(len(''.join(corpus_code[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_corpus(corpus,cut_length):\n",
    "    new_corpus=[]\n",
    "    cut_length=cut_length\n",
    "    print(len(corpus))\n",
    "    for i in range(len(corpus)):\n",
    "        lst=corpus[i]\n",
    "        n=len(lst)\n",
    "        if n<=cut_length:\n",
    "            new_corpus.append(lst)\n",
    "            continue\n",
    "        if n%cut_length==0:\n",
    "            cut_amount=int(n/cut_length)\n",
    "        else:\n",
    "            cut_amount=int((n-n%cut_length)/cut_length)+1\n",
    "        for j in range(cut_amount-1):\n",
    "            new_corpus.append(lst[j*cut_length:(j+1)*cut_length])\n",
    "        new_corpus.append(lst[(cut_amount-1)*cut_length:])\n",
    "    print(len(new_corpus))\n",
    "    return new_corpus\n",
    "corpus_code_cut=cut_corpus(corpus_code,block_size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filament_embeddings=[]\n",
    "for i in range(len(corpus_code_cut)):\n",
    "    if i%200==0:\n",
    "        print(i)\n",
    "    lst=list(np.squeeze(feature_extraction(''.join(corpus_code_cut[i])))[0])\n",
    "    filament_embeddings.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(corpus_code_cut[3938][12:-5]))\n",
    "lst=list(np.squeeze(feature_extraction(''.join(corpus_code_cut[3938])[12:23])))\n",
    "print(len(corpus_code_cut[3938][11:-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(feature_extraction('!32927')[0][6]),np.array(feature_extraction('145980')[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(''.join(corpus_code_cut[77]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans,SpectralClustering,MeanShift, estimate_bandwidth,DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filament_embeddings=np.array(filament_embeddings)\n",
    "mask_1 = np.isfinite(filament_embeddings).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filament_embeddings[mask_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sum = PCA(n_components=2).fit_transform(filament_embeddings)\n",
    "#cluster_pca = KMeans(n_clusters=3).fit_predict(pca_sum[0:len(corpus)])\n",
    "pca_sum_hD = PCA(n_components=30).fit_transform(filament_embeddings)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.scatter(pca_sum[:,0], pca_sum[:,1],alpha=0.6,color='blue')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_pca.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_neighbors=15\n",
    "min_dist=0.1\n",
    "#umap_ND=umap.UMAP(n_neighbors=200,min_dist=0.4,n_components=100).fit_transform(filament_embeddings)\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist)\n",
    "umap_2D = reducer.fit_transform(filament_embeddings)\n",
    "umap_ND=umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,n_components=100).fit_transform(filament_embeddings)\n",
    "print('finish umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+'/'+'umap_3D_bert.npy', 'wb') as f:\n",
    "    np.save(f, umap_ND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filament_cluster_number=4\n",
    "umap_predict=SpectralClustering(n_clusters=filament_cluster_number).fit_predict(umap_ND)\n",
    "#umap_predict=DBSCAN(eps=0.41, min_samples=100).fit_predict(umap_2D)+1\n",
    "#filament_cluster_number=len(np.unique(umap_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "for i in range(filament_cluster_number):\n",
    "    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i)/filament_cluster_number), \n",
    "                                             label=str(i)+' : '+str(np.count_nonzero(umap_predict==i)))\n",
    "plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "#plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c=plt.cm.tab20((umap_predict)/filament_cluster_number))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c='blue')\n",
    "#plt.xlim((-5,15))\n",
    "#plt.ylim((0,15))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_blue_select.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "#c=plt.cm.tab20((umap_predict+1)/filament_cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    cluster_nameID='clusterID'+str(i)\n",
    "    locals()[cluster_name]=[]\n",
    "    locals()[cluster_nameID]=[]\n",
    "data_line=0\n",
    "t=0\n",
    "cluster_choice=umap_predict\n",
    "positive_label=[]\n",
    "for i in range(len(corpus_code_cut)):\n",
    "    lst=corpus_code_cut[i]\n",
    "    cluster_number=cluster_choice[i]\n",
    "    cluster_name='cluster'+str(cluster_number)\n",
    "    cluster_nameID='clusterID'+str(cluster_number)\n",
    "    #print(cluster_name,i)\n",
    "    for j in range(len(lst)):\n",
    "        if lst[j]==encode[word_to_index['0']+5]:\n",
    "            t+=1\n",
    "            continue\n",
    "        locals()[cluster_name].append(data[data_line])\n",
    "        locals()[cluster_nameID].append(data[data_line][8][18:21])\n",
    "        #locals()[cluster_nameID].append(data[data_line][0][67:70])\n",
    "        data_line+=1\n",
    "    positive_label.append(locals()[cluster_nameID][-1])\n",
    "positive_label=np.array(positive_label)\n",
    "labels=list(np.unique(positive_label))\n",
    "positive_label_new=np.array([float(labels.index(x)) for x in positive_label])\n",
    "labels_name=['singlet','doublet'] # define the type of filaments \n",
    "clustersize=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    clustersize.append(len(locals()['cluster'+str(i)]))\n",
    "print(clustersize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_label[-10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(len(labels_name)):\n",
    "    locals()['true_label'+str(i)]=mpatches.Patch(color=plt.cm.tab20(i/3), label=labels_name[i])\n",
    "plt.legend(handles=[eval('true_label'+str(i)) for i in range(len(labels))])\n",
    "print(len(positive_label))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],color=plt.cm.tab20(positive_label_new/3),alpha=0.4)\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_umap_label_bert_select.png\",bbox_inches='tight', pad_inches=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_hist_all=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    distribution_hist=[]\n",
    "    lst=locals()['clusterID'+str(i)]\n",
    "    for j in range(len(labels)):\n",
    "        group_percentage=lst.count(labels[j])/len(lst)\n",
    "        distribution_hist.append(group_percentage)\n",
    "    distribution_hist_all.append(distribution_hist)\n",
    "print(distribution_hist_all)\n",
    "print(len(data),len(cluster0),len(cluster1))\n",
    "\n",
    "fig, ax = plt.subplots(1,filament_cluster_number,figsize = (5*filament_cluster_number,7))\n",
    "\n",
    "for i in range(filament_cluster_number):\n",
    "    ax[i].bar(range(len(labels)),distribution_hist_all[i],tick_label =labels_name)\n",
    "    particle_number=len(locals()['cluster'+str(i)])\n",
    "    ax[i].set_title('cluster{} amount: {}'.format(i,particle_number))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+'distr_new_bert.png')\n",
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    data_cluster=locals()[cluster_name]\n",
    "    if datatype==0:\n",
    "        output=EMdata.output_star(output_path+'/bert_'+file_name,i,data_cluster,metadata)\n",
    "        output.opticgroup(optics)\n",
    "        output.writecluster()\n",
    "    elif datatype==1:\n",
    "        output=EMdata.output_star(output_path+'/'+file_name,i,data_cluster,metadata)\n",
    "        output.writemetadata()\n",
    "        output.writecluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
