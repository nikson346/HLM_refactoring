{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import EMdata\n",
    "import torch\n",
    "import itertools\n",
    "import random, math\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU or CPU\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data path\n",
    "file_path='F:/script/class2vec/real_star_file/synAB_J77/without_pos2/bert_synAB_J77_1.star'\n",
    "datatype=0 #0 is relion 3.1, 1 is relion 3, 2 is cryosparc\n",
    "fast=False\n",
    "block_size=64\n",
    "file_name=os.path.basename(file_path)\n",
    "output_path=os.path.dirname(file_path)+'/'+os.path.splitext(file_name)[0]\n",
    "if os.path.isdir(output_path) is False:\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_rlnImageName', '_rlnDefocusU', '_rlnDefocusV', '_rlnDefocusAngle', '_rlnPhaseShift', '_rlnClassNumber', '_rlnAngleRotPrior', '_rlnAngleTiltPrior', '_rlnAnglePsiPrior', '_rlnAngleRot', '_rlnAngleTilt', '_rlnAnglePsi', '_rlnOriginX', '_rlnOriginY', '_rlnHelicalTubeID', '_rlnOpticsGroup']\n",
      "['000001@/net/jiang/scratch/cryosparc/data/bsakshib/P337/J76/imported/016089425658533590851_000031665775756168679_FoilHole_21951491_Data_21939126_21939128_20220223_233928_EER_patch_aligned_doseweighted.mrcs', '12399.541016', '12399.541016', '46.683114', '0.0', '1', '0.0', '90.0', '111.0', '-90.0', '0.0', '417.86', '-3.58', '-10.08', '1', '1']\n",
      "finish reading\n",
      "number of particles 1942464\n",
      "0 0.30744067827860516 mins\n",
      "10000 0.3094046036402384 mins\n",
      "20000 0.31461392243703207 mins\n",
      "30000 0.3235590179761251 mins\n",
      "40000 0.3355790297190348 mins\n",
      "50000 0.3513289252916972 mins\n",
      "60000 0.3697033882141113 mins\n",
      "70000 0.3920268893241882 mins\n",
      "80000 0.41763918797175087 mins\n",
      "90000 0.44684111277262367 mins\n",
      "100000 0.47912003596623737 mins\n",
      "110000 0.5148827393849691 mins\n",
      "120000 0.5537432034810384 mins\n",
      "130000 0.5962033828099569 mins\n",
      "140000 0.64239635070165 mins\n",
      "150000 0.6912323832511902 mins\n",
      "160000 0.7433451453844706 mins\n",
      "170000 0.7998687624931335 mins\n",
      "180000 0.8601270953814188 mins\n",
      "190000 0.9243170777956645 mins\n",
      "200000 0.9912430008252462 mins\n",
      "210000 1.0621450821558633 mins\n",
      "220000 1.1378661473592122 mins\n",
      "230000 1.2188610434532166 mins\n",
      "240000 1.3040501316388449 mins\n",
      "250000 1.3970443487167359 mins\n",
      "260000 1.4962090452512105 mins\n",
      "270000 1.6024245381355287 mins\n",
      "280000 1.7168253898620605 mins\n",
      "290000 1.8405833760897319 mins\n",
      "300000 1.9773599624633789 mins\n",
      "310000 2.121054697036743 mins\n",
      "320000 2.277890094121297 mins\n",
      "330000 2.4424364884694416 mins\n",
      "340000 2.620951843261719 mins\n",
      "350000 2.8098680853843687 mins\n",
      "360000 3.014517033100128 mins\n",
      "370000 3.230877387523651 mins\n",
      "380000 3.461876904964447 mins\n",
      "390000 3.7037914673487347 mins\n",
      "400000 3.9600370049476625 mins\n",
      "410000 4.23591259320577 mins\n",
      "420000 4.52115375995636 mins\n",
      "430000 4.812092713514963 mins\n",
      "440000 5.1162744919459024 mins\n",
      "450000 5.440164085229238 mins\n",
      "460000 5.773794893423716 mins\n",
      "470000 6.11835488875707 mins\n",
      "480000 6.4775274157524105 mins\n",
      "490000 6.85425242582957 mins\n",
      "500000 7.240098150571187 mins\n",
      "510000 7.636712261041006 mins\n",
      "520000 8.046100334326427 mins\n",
      "530000 8.467971142133077 mins\n",
      "540000 8.903310398260752 mins\n",
      "550000 9.351093089580536 mins\n",
      "560000 9.801285636425018 mins\n",
      "570000 10.266077967484792 mins\n",
      "580000 10.73835205634435 mins\n",
      "590000 11.222301654020946 mins\n",
      "600000 11.71377851565679 mins\n",
      "610000 12.215417285760244 mins\n",
      "620000 12.730551636219024 mins\n",
      "630000 13.25242476463318 mins\n",
      "640000 13.78077967564265 mins\n",
      "650000 14.320692888895671 mins\n",
      "660000 14.868239911397298 mins\n",
      "670000 15.42433550755183 mins\n",
      "680000 15.99114669561386 mins\n",
      "690000 16.564625803629557 mins\n",
      "700000 17.14719142516454 mins\n",
      "710000 17.739738595485687 mins\n",
      "720000 18.34128938515981 mins\n",
      "730000 18.94926369190216 mins\n",
      "740000 19.57063934803009 mins\n",
      "750000 20.196360794703164 mins\n",
      "760000 20.831447883447012 mins\n",
      "770000 21.47604165871938 mins\n",
      "780000 22.130892594655354 mins\n",
      "790000 22.792051565647125 mins\n",
      "800000 23.45933843453725 mins\n",
      "810000 24.13650267124176 mins\n",
      "820000 24.82140302658081 mins\n",
      "830000 25.515724849700927 mins\n",
      "840000 26.218146042029062 mins\n",
      "850000 26.9288733959198 mins\n",
      "860000 27.648338198661804 mins\n",
      "870000 28.376873723665874 mins\n",
      "880000 29.117461319764455 mins\n",
      "890000 29.86322537660599 mins\n",
      "900000 30.617667543888093 mins\n",
      "910000 31.38239405155182 mins\n",
      "920000 32.15364862283071 mins\n",
      "930000 32.929983294010164 mins\n",
      "940000 33.71621052026749 mins\n",
      "950000 34.51497348149618 mins\n",
      "960000 35.32551263570785 mins\n",
      "970000 36.139997378985086 mins\n",
      "980000 36.96629538138708 mins\n",
      "990000 37.797945280869804 mins\n",
      "1000000 38.639726889133456 mins\n",
      "1010000 39.49105130434036 mins\n",
      "1020000 40.35251268545787 mins\n",
      "1030000 41.21979540189107 mins\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if datatype<2:\n",
    "    file_info=EMdata.read_relion(file_path)\n",
    "    if datatype==0:\n",
    "        #read data (relion3.1)\n",
    "        dataset=file_info.getRdata_31()\n",
    "        optics=file_info.extractoptic()\n",
    "    else:\n",
    "        #read relion 3.0\n",
    "        dataset=file_info.getRdata()\n",
    "    metadata=dataset[0]\n",
    "    print(metadata)\n",
    "    data=dataset[1]\n",
    "    print(data[0])\n",
    "    #corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "    #label_path='F:/script/class2vec/real_star_file/self_unsupervised/10230_485_ctf/custom_single/pretext/'\n",
    "    #label=np.load(label_path+'/classes_KM.npy')\n",
    "    if fast:\n",
    "        corpus_information=EMdata.process_helical(dataset).extarct_helical_select_fast()\n",
    "    else:\n",
    "        corpus_information=EMdata.process_helical(dataset).extarct_helical_select()\n",
    "else:\n",
    "    #read cryosparc\n",
    "    dataset=np.load(file_path)\n",
    "    corpus_information=EMdata.process_cryosparc_helical(dataset).extract_helical()\n",
    "corpus_dic,helix_name=corpus_information\n",
    "if fast:\n",
    "    corpus=corpus_dic\n",
    "else:\n",
    "    corpus=list(corpus_dic.values())\n",
    "print(corpus[0])\n",
    "corpus_backup=corpus[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cryosparc\n",
    "#corpus_ignore=corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ignore=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(str(particle[0]))\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(str(particle[0]))\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=['0']\n",
    "                count+=1               \n",
    "    corpus_ignore.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_ignore=corpus\n",
    "print(corpus_ignore[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_length_histogram=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    corpus_length_histogram.append(len(corpus_ignore[i]))\n",
    "plt.hist(corpus_length_histogram,list(range(0,max(corpus_length_histogram)+10,1)))\n",
    "#plt.ylim((0,1000))\n",
    "print(max(corpus_length_histogram))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(itertools.chain.from_iterable(corpus_ignore))\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "if os.path.isdir(output_path+\"/corpus\") is False:\n",
    "    os.mkdir(output_path+\"/corpus\")\n",
    "paths = [str(x) for x in Path(output_path+\"/corpus/\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocabulary_size, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "if os.path.isdir(output_path+\"/tokens\") is False:\n",
    "    os.mkdir(output_path+\"/tokens\")\n",
    "if os.path.isdir(\"./tokens\") is False:\n",
    "    os.mkdir(\"./tokens\")\n",
    "tokenizer.save_model(output_path+\"/tokens\")\n",
    "tokenizer.save_model(\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(output_path+'/tokens/vocab.json') as f:\n",
    "    decode = json.load(f)\n",
    "encode={value:key for (key, value) in decode.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_code=[]\n",
    "for i in range(len(corpus_ignore)):\n",
    "    lst=corpus_ignore[i]\n",
    "    corpus_row=[]\n",
    "    for j in range(len(lst)):\n",
    "        corpus_row.append(encode[word_to_index[lst[j]]+5])\n",
    "    corpus_code.append(corpus_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path+\"/corpus/helical.txt\",\"w\") as f:\n",
    "    for i in range(len(corpus_code)):\n",
    "        lst=corpus_code[i]\n",
    "        for j in range(len(lst)):\n",
    "            if j==len(lst)-1:\n",
    "                f.write(lst[j]+'\\n')\n",
    "            else:\n",
    "                f.write(lst[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '0' in [i for i in vocabulary]:\n",
    "    del decode[encode[word_to_index['0']+5]]\n",
    "    print('delete the ignored group',print(encode[word_to_index['0']+5]))\n",
    "with open('./tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)\n",
    "with open(output_path+'/tokens/vocab.json','w') as f:\n",
    "    json.dump(decode,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(output_path+\"/tokens\", max_len=514)\n",
    "#tokenizer.encode(encode[word_to_index['0']+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=128,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    position_embedding_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "data_import = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=output_path+\"/corpus/helical.txt\",\n",
    "    block_size=block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data_import\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_path+\"/tokens/\")\n",
    "trainer.save_model(\"./tokens/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "feature_extraction = pipeline(\n",
    "    'feature-extraction',model=\"./tokens\",tokenizer=\"./tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(feature_extraction(encode[word_to_index['50']+5])))\n",
    "#print(np.squeeze(feature_extraction('DGG'))[0]-np.squeeze(feature_extraction('JJ'))[0])\n",
    "print(len(''.join(corpus_code[2])))\n",
    "print(len(corpus_ignore[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_corpus(corpus,cut_length):\n",
    "    cut_index=[]\n",
    "    new_corpus=[]\n",
    "    cut_length=cut_length\n",
    "    print(len(corpus))\n",
    "    for i in range(len(corpus)):\n",
    "        lst=corpus[i]\n",
    "        n=len(lst)\n",
    "        if n<=cut_length:\n",
    "            new_corpus.append(lst)\n",
    "            continue\n",
    "        if n%cut_length==0:\n",
    "            cut_amount=int(n/cut_length)\n",
    "        else:\n",
    "            cut_amount=int((n-n%cut_length)/cut_length)+1\n",
    "        for j in range(cut_amount-1):\n",
    "            cut_index.append(i)\n",
    "            new_corpus.append(lst[j*cut_length:(j+1)*cut_length])\n",
    "        new_corpus.append(lst[(cut_amount-1)*cut_length:])\n",
    "    print(len(new_corpus))\n",
    "    return new_corpus,cut_index\n",
    "corpus_code_cut,cut_index=cut_corpus(corpus_code,block_size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filament_embeddings=[]\n",
    "for i in range(len(corpus_code_cut)):\n",
    "    if i%200==0:\n",
    "        print(i)\n",
    "    lst=list(np.squeeze(feature_extraction(''.join(corpus_code_cut[i])))[0])\n",
    "    filament_embeddings.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans,SpectralClustering,MeanShift, estimate_bandwidth,AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import multivariate_normal \n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filament_embeddings=np.array(filament_embeddings)\n",
    "mask_1 = np.isfinite(filament_embeddings).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filament_embeddings[mask_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sum = PCA(n_components=2).fit_transform(filament_embeddings)\n",
    "#cluster_pca = KMeans(n_clusters=3).fit_predict(pca_sum[0:len(corpus)])\n",
    "pca_sum_hD = PCA(n_components=30).fit_transform(filament_embeddings)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.scatter(pca_sum[:,0], pca_sum[:,1],alpha=0.6,color='blue')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_pca.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_neighbors=15\n",
    "min_dist=0.1\n",
    "#umap_ND=umap.UMAP(n_neighbors=200,min_dist=0.4,n_components=100).fit_transform(filament_embeddings)\n",
    "reducer = umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,metric='cosine')\n",
    "umap_2D = reducer.fit_transform(filament_embeddings)\n",
    "umap_ND=umap.UMAP(n_neighbors=n_neighbors,min_dist=min_dist,n_components=100).fit_transform(filament_embeddings)\n",
    "print('finish umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "n_cluster = range(1,20)\n",
    "for n in n_cluster:\n",
    "    print(n)\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(umap_ND)\n",
    "    res.append(np.average(np.min(cdist(umap_ND, kmeans.cluster_centers_, 'euclidean'), axis=1)))\n",
    "        \n",
    "plt.plot(n_cluster, res)\n",
    "plt.title('elbow curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "n_cluster = range(2,8)\n",
    "for i in n_cluster:\n",
    "    print(i)\n",
    "    kmeans_model = KMeans(n_clusters=i).fit(umap_2D)\n",
    "    labels = kmeans_model.labels_\n",
    "    a=metrics.silhouette_score(umap_ND, labels, metric='euclidean')\n",
    "    res.append(a)\n",
    "\n",
    "plt.plot(n_cluster, res)\n",
    "print(res)\n",
    "plt.title('silhouette curve')\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_silhouette.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filament_cluster_number=res.index(max(res[1:]))+2\n",
    "#filament_cluster_number=5\n",
    "#print(filament_cluster_number)\n",
    "#umap_predict=KMeans(n_clusters=filament_cluster_number).fit_predict(umap_2D)\n",
    "#umap_predict=SpectralClustering(n_clusters=filament_cluster_number).fit_predict(umap_ND)\n",
    "umap_predict=DBSCAN(eps=0.7, min_samples=300).fit_predict(umap_2D)+1\n",
    "filament_cluster_number=len(np.unique(umap_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "for i in range(filament_cluster_number):\n",
    "    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i+1)/filament_cluster_number), \n",
    "                                             label=str(i)+' : '+str(np.count_nonzero(umap_predict==i)))\n",
    "plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "#plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c=plt.cm.tab20((umap_predict)/filament_cluster_number))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.7,c=plt.cm.tab20((umap_predict+1)/filament_cluster_number),s=2)\n",
    "#plt.xlim((-5,10))\n",
    "#plt.ylim((-10,6))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_np.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "#c=plt.cm.tab20((umap_predict+1)/filament_cluster_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "#for i in range(filament_cluster_number):\n",
    "#    locals()['labels'+str(i)]=mpatches.Patch(color=plt.cm.tab20((i)/filament_cluster_number), \n",
    "#                                             label=str(i)+' : '+str(np.count_nonzero(umap_predict==i)))\n",
    "#plt.legend(handles=[eval('labels'+str(i)) for i in range(filament_cluster_number)])\n",
    "#plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.6,c=plt.cm.tab20((umap_predict)/filament_cluster_number))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],alpha=0.5,c='blue')\n",
    "plt.xlim((-5,10))\n",
    "plt.ylim((-3,12))\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_bert_umap_blue.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()\n",
    "#c=plt.cm.tab20((umap_predict+1)/filament_cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    cluster_nameID='clusterID'+str(i)\n",
    "    locals()[cluster_name]=[]\n",
    "    locals()[cluster_nameID]=[]\n",
    "\n",
    "cluster_choice=umap_predict\n",
    "positive_label=[]\n",
    "cut_index=np.array(cut_index)\n",
    "for i in range(len(helix_name)):\n",
    "    if i in cut_index:\n",
    "        t=np.count_nonzero(cut_index==i)\n",
    "        while t>0:\n",
    "            positive_label.append(helix_name[i][11:14])\n",
    "            t-=1\n",
    "    positive_label.append(helix_name[i][11:14])\n",
    "positive_label=np.array(positive_label)\n",
    "labels=list(np.unique(positive_label))\n",
    "positive_label_new=np.array([float(labels.index(x)) for x in positive_label])\n",
    "#labels_name=['type 3','type 1B','type 2B','type 1A','type 2A','type 2AB'] # define the type of filaments \n",
    "labels_name=['singlet','doublet']\n",
    "#labels_name=['1','2']\n",
    "#labels_name=['1','2','3','4','5']\n",
    "#labels_name=['SF','PHF']\n",
    "clustersize=[]\n",
    "for i in range(filament_cluster_number):\n",
    "    clustersize.append(len(locals()['cluster'+str(i)]))\n",
    "print(positive_label_new[2])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helix_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(cut_index==126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "for i in range(len(labels_name)):\n",
    "    locals()['true_label'+str(i)]=mpatches.Patch(color=plt.cm.tab20(i/3), label=labels_name[i])\n",
    "plt.legend(handles=[eval('true_label'+str(i)) for i in range(len(labels))])\n",
    "print(len(positive_label))\n",
    "#plt.xlim((2,17))\n",
    "#plt.ylim((0,15))\n",
    "plt.scatter(umap_2D[:,0], umap_2D[:,1],color=plt.cm.tab20(positive_label_new/3),alpha=0.4)\n",
    "plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+\"_label_bert.png\",bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive_label_new),len(cluster_choice))\n",
    "hist_data=pd.DataFrame({'labels':positive_label_new,'predict':cluster_choice})\n",
    "distribution_hist_all=[]\n",
    "print(hist_data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,filament_cluster_number):\n",
    "#    distribution=[]\n",
    "#    lst=hist_data[hist_data['predict']==i]\n",
    "#    for j in range(len(labels)):\n",
    "#        distribution.append(len(lst[lst['labels']==j])/len(lst))\n",
    "#    distribution_hist_all.append(distribution)\n",
    "#print(distribution_hist_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1,filament_cluster_number,figsize = (5*filament_cluster_number,7))\n",
    "#for i in range(filament_cluster_number):\n",
    "#    ax[i].bar(range(len(labels)),distribution_hist_all[i],tick_label =labels_name)\n",
    "#    particle_number=len(locals()['cluster'+str(i)])\n",
    "#    ax[i].set_title('cluster{} amount: {}'.format(i,particle_number))\n",
    "#\n",
    "#plt.savefig(output_path+'/'+os.path.splitext(file_name)[0]+'distr_new_bert.png')\n",
    "#plt.show()\n",
    "#print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill corpus\n",
    "corpus_fill=[]\n",
    "for i in range(len(corpus)):\n",
    "    corpus_row=[]\n",
    "    count=1\n",
    "    lst=corpus[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if j==0:\n",
    "            count+=particle[1]-1\n",
    "        if count==int(particle[1]):\n",
    "            corpus_row.append(particle)\n",
    "            count+=1\n",
    "        else:\n",
    "            while 1:\n",
    "                if count==int(lst[j][1]):\n",
    "                    corpus_row.append(particle)\n",
    "                    count+=1\n",
    "                    break\n",
    "                corpus_row+=[(0,count,0)]\n",
    "                count+=1\n",
    "    corpus_fill.append(corpus_row)\n",
    "corpus_fill_cut,corpus_cut_index=cut_corpus(corpus_fill,block_size-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    locals()['cluster'+str(i)]=[]\n",
    "    locals()['clusterID'+str(i)]=[]\n",
    "#count=0    \n",
    "for i in range(len(corpus_fill_cut)):\n",
    "    labels=umap_predict[i]\n",
    "    locals()['clusterID'+str(labels)].append(i)\n",
    "    lst=corpus_fill_cut[i]\n",
    "    for j in range(len(lst)):\n",
    "        particle=lst[j]\n",
    "        if particle[0]!=0:\n",
    "            #count+=1\n",
    "            dataline=particle[-1]\n",
    "            locals()['cluster'+str(labels)].append(data[dataline])\n",
    "number=0\n",
    "for i in range(filament_cluster_number):\n",
    "    cluster_number_count=len(locals()['cluster'+str(i)])\n",
    "    print(i,cluster_number_count)\n",
    "    number=number+cluster_number_count\n",
    "print(number-len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input label\n",
    "#for i in range(filament_cluster_number):\n",
    "#    locals()['cluster'+str(i)]=[]\n",
    "#    locals()['clusterID'+str(i)]=[]\n",
    "#for i in range(len(corpus)):\n",
    "#    labels=umap_predict[i]\n",
    "#    locals()['clusterID'+str(labels)].append(i)\n",
    "#    lst=corpus[i]\n",
    "#    for j in range(len(lst)):\n",
    "#        dataline=i+j\n",
    "#        locals()['cluster'+str(labels)].append(data[dataline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filament_cluster_number):\n",
    "    cluster_name='cluster'+str(i)\n",
    "    data_cluster=locals()[cluster_name]\n",
    "    if datatype==0:\n",
    "        output=EMdata.output_star(output_path+'/bert_'+file_name,i,data_cluster,metadata)\n",
    "        output.opticgroup(optics)\n",
    "        output.writecluster()\n",
    "    elif datatype==1:\n",
    "        output=EMdata.output_star(output_path+'/'+file_name,i,data_cluster,metadata)\n",
    "        output.writemetadata()\n",
    "        output.writecluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
